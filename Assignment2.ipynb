{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class SpellCorrection:\n",
        "    def __init__(self, data_path):\n",
        "        self.bigram = self.bigram_train(data_path)\n",
        "        self.vocab = self.vocab_collect(data_path)\n",
        "\n",
        "    def vocab_collect(self, data_path):\n",
        "        vocab = set()\n",
        "        with open(data_path, 'r', encoding='ISO-8859-1') as f:\n",
        "            for line in f.readlines():\n",
        "                if len(line.split()) == 3:\n",
        "                    freq, word1, word2 = line.split()\n",
        "                    vocab.add(word1)\n",
        "                    vocab.add(word2)\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def bigram_train(self, data_path):\n",
        "        bigram_prob = {}\n",
        "        with open(data_path, 'r', encoding='ISO-8859-1') as f:\n",
        "            bigram = {}\n",
        "            total_pairs = 0\n",
        "            for line in f.readlines():\n",
        "                if len(line.split()) == 3:\n",
        "                    freq, word1, word2 = line.split()\n",
        "\n",
        "                    if (word1, word2) not in bigram:\n",
        "                        bigram[(word1, word2)] = 0\n",
        "\n",
        "                    bigram[(word1, word2)] += (int)(freq)\n",
        "                    total_pairs += (int)(freq)\n",
        "            bigram_prob = {key: value / total_pairs for key, value in bigram.items()}\n",
        "\n",
        "        return bigram_prob\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"All edits that are one edit away from `word`. Source https://norvig.com/spell-correct.html\"\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def editsK(self, word, k):\n",
        "        if k < 0:\n",
        "            print(f\"Parametr k={k} must be >= 0\")\n",
        "            return set()\n",
        "        if k == 0:\n",
        "            return set(word)\n",
        "        if k == 1:\n",
        "            return self.edits1(word[0])\n",
        "        else:\n",
        "            edits_k = self.edits1(word[0])\n",
        "            for word_k in self.edits1(word[0]):\n",
        "                edits_k = edits_k.union(self.editsK([word_k], k - 1))\n",
        "\n",
        "            return edits_k\n",
        "\n",
        "    def known(self, words):\n",
        "        return set(word for word in words if word in self.vocab)\n",
        "\n",
        "    def candidates(self, word, K=1):\n",
        "        candidat = set()\n",
        "        for k in range(K + 1):\n",
        "            candidat = candidat.union(self.known(self.editsK([word], k)))\n",
        "\n",
        "        return candidat\n",
        "\n",
        "    def correction(self, sentence, K=1, print_prob=False):\n",
        "        words = re.findall('\\w+', sentence.lower())\n",
        "        if len(words) == 0:\n",
        "            return \"\"\n",
        "\n",
        "        last_word = words[0]\n",
        "        correct_sentence = \"\"\n",
        "        for word in words:\n",
        "            if len(correct_sentence) > 0:\n",
        "                correct_sentence += \" \"\n",
        "            if word in self.vocab:\n",
        "                correct_sentence += word\n",
        "            else:\n",
        "                candidates = list(self.candidates(word, K))\n",
        "                if len(candidates) == 0:\n",
        "                    correct_sentence += \" \" + word\n",
        "                    continue\n",
        "                probs = [self.bigram.get((last_word, candidate), 1e-10) for candidate in candidates]\n",
        "                if (print_prob):\n",
        "                    print(f\"Try to change word={word}\")\n",
        "                argmax = (candidates[0], probs[0])\n",
        "                for candidate, prob in zip(candidates, probs):\n",
        "                    if print_prob:\n",
        "                        print(f\"Candidate={candidate}, prob={prob}\")\n",
        "                    if prob > argmax[1]:\n",
        "                        argmax = (candidate, prob)\n",
        "                correct_sentence += argmax[0]\n",
        "            last_word = word\n",
        "\n",
        "        return correct_sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spell_checker = SpellCorrection('./bigrams.txt')"
      ],
      "metadata": {
        "id": "wLBzGWAeNOyr"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spell_checker.correction('today i saw a dking species', print_prob=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "y4aN4pylPDOF",
        "outputId": "f6e686e0-6daf-497a-d8fa-41e2cf9e79e1"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Try to change word=dking\n",
            "Candidate=ding, prob=5.416526363587943e-08\n",
            "Candidate=king, prob=2.215359282707469e-06\n",
            "Candidate=duking, prob=1e-10\n",
            "Candidate=doing, prob=1e-10\n",
            "Candidate=dying, prob=1.6339854530156962e-06\n",
            "Candidate=eking, prob=1e-10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'today i saw a king species'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asnwer\n",
        "\n",
        "- To solve this task, i choose bigram dataset, and train bigram based on this data.\n",
        "- Weights for each edits are equal, and if you want you play with k, which stand for k recursive to combine all words which their distance is at most k"
      ],
      "metadata": {
        "id": "e0sT3UWFTx-w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Evaluation code took from https://norvig.com/spell-correct.html\n",
        "def spelltest(model, tests, verbose=False):\n",
        "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
        "    start = time.time()\n",
        "    good, unknown = 0, 0\n",
        "    n = len(tests)\n",
        "    for right, wrong in tests:\n",
        "        w = model.correction(wrong)\n",
        "        good += (w == right)\n",
        "        if w != right:\n",
        "            unknown += (right not in model.vocab)\n",
        "            if verbose:\n",
        "                print('correction({}) => {}; expected {}'\n",
        "                      .format(wrong, w, right))\n",
        "    dt = time.time() - start\n",
        "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
        "          .format(good / n, n, unknown / n, n / dt))\n",
        "\n",
        "def Testset(lines):\n",
        "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
        "    return [(right, wrong)\n",
        "            for (right, wrongs) in (line.split(':') for line in lines)\n",
        "            for wrong in wrongs.split()]"
      ],
      "metadata": {
        "id": "xP99_t5GUS2Z"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spelltest(spell_checker, Testset(open('spell-testset1.txt')))\n",
        "spelltest(spell_checker, Testset(open('spell-testset2.txt')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFbQBVHMUlPq",
        "outputId": "63f0e24e-d0ed-4409-fae1-8a8930aaf89a"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53% of 270 correct (3% unknown) at 4969 words per second \n",
            "56% of 400 correct (4% unknown) at 4920 words per second \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems data taht I took from https://norvig.com/spell-correct.html, is more related to single words, while bigram that I trained is more relate to 2 words text, hence low performance."
      ],
      "metadata": {
        "id": "obcdr45lVaVM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}